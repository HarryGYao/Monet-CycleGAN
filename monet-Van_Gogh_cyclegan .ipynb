{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction and Setup\n\nOur problem may be thought of as an image-to-image conversion. The purpose is to learn a mapping G: X->Y that converts an image x in X to y in Y. We will be building an architecture similar to that of CycleGAN, using the U-net architecture in the generator.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport seaborn as sns\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, random, json, PIL, shutil, re, imageio, glob\nfrom tensorflow.keras.callbacks import Callback\nfrom glob import glob\nimport cv2 \n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntf.random.set_seed(0)\n    \nprint(tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-03-12T09:13:43.127069Z","iopub.execute_input":"2022-03-12T09:13:43.12749Z","iopub.status.idle":"2022-03-12T09:13:51.359293Z","shell.execute_reply.started":"2022-03-12T09:13:43.12746Z","shell.execute_reply":"2022-03-12T09:13:51.358436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in the data\n\nFirst, load in the Monet painting (TFRecords).","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('monet-gan-getting-started')\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nn_monet_samples = count_data_items(MONET_FILENAMES)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:09.894206Z","iopub.execute_input":"2022-03-12T05:20:09.894693Z","iopub.status.idle":"2022-03-12T05:20:10.013087Z","shell.execute_reply.started":"2022-03-12T05:20:09.894646Z","shell.execute_reply":"2022-03-12T05:20:10.012253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\nBUFFER_SIZE = 1000\nBATCH_SIZE =  4\nEPOCHS_NUM = 15\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nEPOCHS = 15\n\ndef decode_image(image): \n    image = tf.image.decode_jpeg(image, channels=3)#Decode a JPEG-encoded image to a uint8 tensor. contents:A Tensor of type string,所以下面用的tf.string\n    image = (tf.cast(image, tf.float32) / 127.5) - 1 #unit8: Unsigned Integers of 8 bits. \n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example): \n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)#（A scalar string Tensor, a single serialized Example，A dict mapping feature keys to FixedLenFeature or VarLenFeature values.）\n    #Return A dict mapping feature keys to Tensor and SparseTensor values. \n    image = decode_image(example['image'])\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.020246Z","iopub.execute_input":"2022-03-12T05:20:10.020631Z","iopub.status.idle":"2022-03-12T05:20:10.032762Z","shell.execute_reply.started":"2022-03-12T05:20:10.020601Z","shell.execute_reply":"2022-03-12T05:20:10.032045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames) #A Dataset comprising records from one or more TFRecord files. dataset: tf.data.Dataset\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)#好家伙还是得看看TPU是个啥。 但先猜这行是用来TPU提速的。\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.05141Z","iopub.execute_input":"2022-03-12T05:20:10.051648Z","iopub.status.idle":"2022-03-12T05:20:10.064927Z","shell.execute_reply.started":"2022-03-12T05:20:10.051622Z","shell.execute_reply":"2022-03-12T05:20:10.064216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1) \nmonet_file  = load_dataset(MONET_FILENAMES, labeled=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.067516Z","iopub.execute_input":"2022-03-12T05:20:10.068107Z","iopub.status.idle":"2022-03-12T05:20:10.419532Z","shell.execute_reply.started":"2022-03-12T05:20:10.068073Z","shell.execute_reply":"2022-03-12T05:20:10.418627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we load Van Gogh painting(JPGs)","metadata":{}},{"cell_type":"code","source":"main_path = '/kaggle/input/cyclegan-model/van-gogh-paintings/'\nstyle_img_paths = []\nfor class_path in [os.path.join(main_path,class_name) for class_name in os.listdir(main_path)]:\n    \n    class_img_paths = glob(class_path+\"/*\")\n    for class_img_path in class_img_paths:\n        style_img_paths.append(class_img_path)\n\nprint(\"There are {} style images in Van Gogh Paintings Dataset\".format(len(style_img_paths)))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:19:37.546364Z","iopub.execute_input":"2022-03-12T05:19:37.54665Z","iopub.status.idle":"2022-03-12T05:19:37.621097Z","shell.execute_reply.started":"2022-03-12T05:19:37.546622Z","shell.execute_reply":"2022-03-12T05:19:37.620425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_images = []\nfor style_path in style_img_paths:\n    img = cv2.imread(style_path)\n    img = cv2.resize(img,(256,256))\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    style_images.append(img)\n    \nn_van_samples = len(style_images)\nn_van_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:19:37.622287Z","iopub.execute_input":"2022-03-12T05:19:37.622513Z","iopub.status.idle":"2022-03-12T05:20:06.667917Z","shell.execute_reply.started":"2022-03-12T05:19:37.622489Z","shell.execute_reply":"2022-03-12T05:20:06.667187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_images = np.array(style_images,dtype=np.float32)\n# scaling between -1 and 1\nstyle_images = style_images / 127.5 - 1\n# batching\nvan_file = tf.data.Dataset.from_tensor_slices(style_images)\nvan_ds = tf.data.Dataset.from_tensor_slices(style_images).batch(1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:06.669334Z","iopub.execute_input":"2022-03-12T05:20:06.669576Z","iopub.status.idle":"2022-03-12T05:20:09.892039Z","shell.execute_reply.started":"2022-03-12T05:20:06.669551Z","shell.execute_reply":"2022-03-12T05:20:09.891387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image):\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    # Apply jitter\n    if p_crop > .5:\n        image = tf.image.resize(image, [286, 286])\n        image = tf.image.random_crop(image, size=[256, 256, 3])\n        if p_crop > .9:\n            image = tf.image.resize(image, [300, 300])\n            image = tf.image.random_crop(image, size=[256, 256, 3])\n    \n    # Random rotation\n    if p_rotate > .9:\n        image = tf.image.rot90(image, k=3) # rotate 270º\n    elif p_rotate > .7:\n        image = tf.image.rot90(image, k=2) # rotate 180º\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=1) # rotate 90º\n    \n    # Random mirroring\n    if p_spatial > .6:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        if p_spatial > .9:\n            image = tf.image.transpose(image)\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.034558Z","iopub.execute_input":"2022-03-12T05:20:10.035266Z","iopub.status.idle":"2022-03-12T05:20:10.050001Z","shell.execute_reply.started":"2022-03-12T05:20:10.03522Z","shell.execute_reply":"2022-03-12T05:20:10.049227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gan_dataset(monet_file, van_file,augment=None, repeat=True, shuffle=True, batch_size=1):\n    \n    if augment:\n        monet_file = monet_file.map(augment, num_parallel_calls=AUTOTUNE)\n        van_file = van_file.map(augment, num_parallel_calls=AUTOTUNE)\n\n    if repeat:\n        monet_file = monet_file.repeat()\n        van_file = van_file.repeat()\n        \n    if shuffle:\n        monet_file = monet_file.shuffle(1024, reshuffle_each_iteration=True)\n        van_file = van_file.shuffle(1024, reshuffle_each_iteration=True)\n        \n    monet_file = monet_file.batch(batch_size, drop_remainder=True)\n    van_file = van_file.batch(batch_size, drop_remainder=True)\n    van_file = van_file.cache()\n    monet_file = monet_file.cache()\n    monet_file = monet_file.prefetch(AUTOTUNE)\n    van_file = van_file.prefetch(AUTOTUNE)\n    \n    gan_ds = tf.data.Dataset.zip((monet_file, van_file))\n    \n    return gan_ds","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.432206Z","iopub.execute_input":"2022-03-12T05:20:10.432506Z","iopub.status.idle":"2022-03-12T05:20:10.446051Z","shell.execute_reply.started":"2022-03-12T05:20:10.432467Z","shell.execute_reply":"2022-03-12T05:20:10.445362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset = get_gan_dataset(monet_file, van_file, augment=data_augment, repeat=True, shuffle=True, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:10.447607Z","iopub.execute_input":"2022-03-12T05:20:10.44791Z","iopub.status.idle":"2022-03-12T05:20:11.253029Z","shell.execute_reply.started":"2022-03-12T05:20:10.447873Z","shell.execute_reply":"2022-03-12T05:20:11.252133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet , example_van_goph = next(iter(full_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:11.254548Z","iopub.execute_input":"2022-03-12T05:20:11.255052Z","iopub.status.idle":"2022-03-12T05:20:14.64419Z","shell.execute_reply.started":"2022-03-12T05:20:11.254997Z","shell.execute_reply":"2022-03-12T05:20:14.643376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's  visualize a van example and a Monet example.","metadata":{}},{"cell_type":"code","source":"def view_image(ds, nrows=1, ncols=5):\n    ds_iter = iter(ds)\n    # image = next(iter(ds)) # extract 1 from the dataset\n    # image = image.numpy()  # convert the image tensor to NumPy ndarrays.\n\n    fig = plt.figure(figsize=(25, nrows * 5.05 )) # figsize with Width, Height\n    \n    # loop thru all the images (number of rows * number of columns)\n    for i in range(ncols * nrows):\n        image = next(ds_iter)\n        image = image.numpy()\n        ax = fig.add_subplot(nrows, ncols, i+1, xticks=[], yticks=[])\n        ax.imshow(image[0] * 0.5 + .5) # rescale the data in [0, 1] for display","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:14.645266Z","iopub.execute_input":"2022-03-12T05:20:14.646022Z","iopub.status.idle":"2022-03-12T05:20:14.652088Z","shell.execute_reply.started":"2022-03-12T05:20:14.645984Z","shell.execute_reply":"2022-03-12T05:20:14.651408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_image(monet_ds,2, 5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:14.653251Z","iopub.execute_input":"2022-03-12T05:20:14.653551Z","iopub.status.idle":"2022-03-12T05:20:15.858683Z","shell.execute_reply.started":"2022-03-12T05:20:14.653511Z","shell.execute_reply":"2022-03-12T05:20:15.857715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_image(van_ds,2, 5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:15.860204Z","iopub.execute_input":"2022-03-12T05:20:15.860834Z","iopub.status.idle":"2022-03-12T05:20:17.300012Z","shell.execute_reply.started":"2022-03-12T05:20:15.860787Z","shell.execute_reply":"2022-03-12T05:20:17.299349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the generator\n\nOur problem may be thought of as an image-to-image conversion. The purpose is to learn a mapping G: X->Y that converts an image x in X to y in Y. We will be building an architecture similar to that of CycleGAN, using the U-net architecture in the generator.\nOur goal is to train two generator models using the CycleGAN architecture. The two generator models generate images from the input domain to the target domain and back. We also train two discriminator models to differentiate the images in the two domains. The ultimate goal is for the two generator models to generate images that cannot be distinguished by the discriminator models. In addition, we would also like to ensure cycle consistency by making sure that an image can be transformed from the input style to the target style and back again. This cycle consistency ensures that we have x -> G_X(x) -> G_Y(G_X(x)) -> x, where x is an image in the input domain, G_X is the generator model that generates a target image from the input image, and G_Y is the reverse generator model that generates an input image from the target image. Cycle consistency ensures that the input image matches the final output image by applying two generator transformations.\n","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size,stride=2, apply_instancenorm=True):#filter = Integer, the dimensionality of the output space,size = kernel size\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02) #Initializer that generates tensors with a normal distribution\n \n    result = keras.Sequential() #Sequential groups a linear stack of layers into a tf.keras.Model.\n    #from tensorflow.keras import layers\n    result.add(layers.Conv2D(filters, size, strides=stride, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)) #Instance normalization layer. Inherits From: GroupNormalization\n        #tfa.layers.GroupNormalization:Group normalization layer.\n    \n\n    result.add(layers.LeakyReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:17.301298Z","iopub.execute_input":"2022-03-12T05:20:17.30155Z","iopub.status.idle":"2022-03-12T05:20:17.31046Z","shell.execute_reply.started":"2022-03-12T05:20:17.301521Z","shell.execute_reply":"2022-03-12T05:20:17.308678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(filters, size,stride=2, apply_dropout=False):#小变大\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    #tf.keras.layers.Conv2DTranspose: Transposed convolution layer (sometimes called Deconvolution).\n    result.add(layers.Conv2DTranspose(filters, size, strides=stride, \n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:17.312054Z","iopub.execute_input":"2022-03-12T05:20:17.312563Z","iopub.status.idle":"2022-03-12T05:20:17.323723Z","shell.execute_reply.started":"2022-03-12T05:20:17.312486Z","shell.execute_reply":"2022-03-12T05:20:17.322734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the generator, we plan to apply seven convolution layers with a kernel size of 4 for the downsampling, seven transpose convolution layers with a kernel size of 4 for the upsampling, and concatenate the downsampling and upsampling layers in the same depth.","metadata":{}},{"cell_type":"code","source":"def Generator():\n    inputs = layers.Input(shape=[256,256,3]) #256*256*3\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 7, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]#conv2d layer\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]#Tansposeconv2d layer\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 7,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skip_connection = 7\n    skips = []\n    for down in down_stack: #down \n        x = down(x) \n        skips.append(x)\n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        if skip_connection <= 4:\n            x = layers.Concatenate()([x, skip])\n        skip_connection = skip_connection-1\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:17.32577Z","iopub.execute_input":"2022-03-12T05:20:17.326434Z","iopub.status.idle":"2022-03-12T05:20:17.356568Z","shell.execute_reply.started":"2022-03-12T05:20:17.326358Z","shell.execute_reply":"2022-03-12T05:20:17.355428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the discriminator\n\nSince a powerful discriminator will cause low performance for the whole model, it will be a simple convolution neural network.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4,2, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4,2)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4,2)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) \n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:17.358338Z","iopub.execute_input":"2022-03-12T05:20:17.358562Z","iopub.status.idle":"2022-03-12T05:20:17.380324Z","shell.execute_reply.started":"2022-03-12T05:20:17.358538Z","shell.execute_reply":"2022-03-12T05:20:17.379388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): \n    monet_generator = Generator() # transforms vans to Monet-esque paintings\n    van_generator = Generator() # transforms Monet paintings to be more like vans\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    van_discriminator = Discriminator() # differentiates real vans and generated vans","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:17.385536Z","iopub.execute_input":"2022-03-12T05:20:17.386401Z","iopub.status.idle":"2022-03-12T05:20:35.983415Z","shell.execute_reply.started":"2022-03-12T05:20:17.386354Z","shell.execute_reply":"2022-03-12T05:20:35.98261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the CycleGAN model\n\n","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        van_generator,\n        monet_discriminator,\n        van_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__() \n        self.m_gen = monet_generator\n        self.p_gen = van_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = van_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile( #Configures the model for training.\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_van = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape: #Record operations for automatic differentiation. persistent=True \n            #Boolean controlling whether a persistent gradient tape is created. False by default, \n            #which means at most one call can be made to the gradient() method on this object.\n            \n            # van to monet back to van  \n            fake_monet = self.m_gen(real_van, training=True)\n            cycled_van = self.p_gen(fake_monet, training=True)\n\n            # monet to van back to monet\n            fake_van = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_van, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_van = self.p_gen(real_van, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_van = self.p_disc(real_van, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_van = self.p_disc(fake_van, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            van_gen_loss = self.gen_loss_fn(disc_fake_van)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_van, cycled_van, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_van_gen_loss = van_gen_loss + total_cycle_loss + self.identity_loss_fn(real_van, same_van, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            van_disc_loss = self.disc_loss_fn(disc_real_van, disc_fake_van)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        van_generator_gradients = tape.gradient(total_van_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        van_discriminator_gradients = tape.gradient(van_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(van_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(van_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"van_gen_loss\": total_van_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"van_disc_loss\": van_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:47.762484Z","iopub.execute_input":"2022-03-12T05:20:47.762756Z","iopub.status.idle":"2022-03-12T05:20:47.784313Z","shell.execute_reply.started":"2022-03-12T05:20:47.76273Z","shell.execute_reply":"2022-03-12T05:20:47.783423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss functions\n\nDue to the lack of paired data to train on in CycleGAN, there is no guarantee that the input x and target y pair will be relevant throughout training. The author from the original paper suggested we use the cycle consistency loss (the output should be close to the original input) to ensure that the network learns the correct mapping. Therefore, for the generator, we will compare the result with the value of the true label (suppose we use 1) to let the generator learn how to make the target image. For the discriminator, we will compare the real photo with the value of the true label and the generated photo with the value of the fake label (suppose we use 0) since we want the discriminator to learn to distinguish between the true and fake. For the current stage, we plan to use BinaryCrossEntropy to calculate the loss. We will use Adam as our optimizer.\n","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        \n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:50.278408Z","iopub.execute_input":"2022-03-12T05:20:50.278979Z","iopub.status.idle":"2022-03-12T05:20:50.286024Z","shell.execute_reply.started":"2022-03-12T05:20:50.278943Z","shell.execute_reply":"2022-03-12T05:20:50.285093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n       # return tf.square(tf.ones_like(generated) - generated)\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:52.167037Z","iopub.execute_input":"2022-03-12T05:20:52.167988Z","iopub.status.idle":"2022-03-12T05:20:52.172754Z","shell.execute_reply.started":"2022-03-12T05:20:52.16795Z","shell.execute_reply":"2022-03-12T05:20:52.172193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        #loss1 = tf.reduce_mean(tf.square(real_image - cycled_image))\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:53.348615Z","iopub.execute_input":"2022-03-12T05:20:53.348921Z","iopub.status.idle":"2022-03-12T05:20:53.353506Z","shell.execute_reply.started":"2022-03-12T05:20:53.348888Z","shell.execute_reply":"2022-03-12T05:20:53.352861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        #loss = tf.reduce_mean(tf.square(real_image - same_image))\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:54.979818Z","iopub.execute_input":"2022-03-12T05:20:54.980147Z","iopub.status.idle":"2022-03-12T05:20:54.984872Z","shell.execute_reply.started":"2022-03-12T05:20:54.980088Z","shell.execute_reply":"2022-03-12T05:20:54.984241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the CycleGAN\n\n","metadata":{}},{"cell_type":"markdown","source":"From the original paper, a small learning rate at the end of the training will be helpful","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef linear_schedule_with_warmup(step):\n    \"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"\n    lr_start   = 2e-4\n    lr_max     = 2e-4\n    lr_min     = 0.\n    \n    steps_per_epoch = int(max(n_monet_samples, n_van_samples)//BATCH_SIZE)\n    total_steps = EPOCHS * steps_per_epoch\n    warmup_steps = 1\n    hold_max_steps = total_steps * 0.8\n    \n    if step < warmup_steps:\n        lr = (lr_max - lr_start) / warmup_steps * step + lr_start\n    elif step < warmup_steps + hold_max_steps:\n        lr = lr_max\n    else:\n        lr = lr_max * ((total_steps - step) / (total_steps - warmup_steps - hold_max_steps))\n        if lr_min is not None:\n            lr = tf.math.maximum(lr_min, lr)\n\n    return lr\n\nsteps_per_epoch = int(max(n_monet_samples, n_van_samples)//BATCH_SIZE)\ntotal_steps = EPOCHS * steps_per_epoch\nrng = [i for i in range(0, total_steps, 50)]\ny = [linear_schedule_with_warmup(x) for x in rng]\n\nsns.set(style=\"whitegrid\")\nfig, ax = plt.subplots(figsize=(20, 6))\nplt.plot(rng, y)\nprint(f'{EPOCHS} total epochs and {steps_per_epoch} steps per epoch')\nprint(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:56.658995Z","iopub.execute_input":"2022-03-12T05:20:56.659321Z","iopub.status.idle":"2022-03-12T05:20:57.462573Z","shell.execute_reply.started":"2022-03-12T05:20:56.659287Z","shell.execute_reply":"2022-03-12T05:20:57.46004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callbacks\nclass GANMonitor(Callback):\n    \"\"\"A callback to generate and save images after each epoch\"\"\"\n\n    def __init__(self, num_img=1, monet_path='monet', van_path='van'):\n        self.num_img = num_img\n        self.monet_path = monet_path\n        self.van_path = van_path\n        self.epoch = 0\n        self.N = 10\n        # Create directories to save the generate images\n        if not os.path.exists(self.monet_path):\n            os.makedirs(self.monet_path)\n        if not os.path.exists(self.van_path):\n            os.makedirs(self.van_path)\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Monet generated images\n        for i, img in enumerate(van_ds.take(self.num_img)):\n            prediction = monet_generator(img, training=False)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            prediction = PIL.Image.fromarray(prediction)\n            prediction.save(f'{self.monet_path}/generated_{i}_{epoch+1}.png')\n            \n        # van generated images\n        for i, img in enumerate(monet_ds.take(self.num_img)):\n            prediction = van_generator(img, training=False)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            prediction = PIL.Image.fromarray(prediction)\n            prediction.save(f'{self.van_path}/generated_{i}_{epoch+1}.png')\n            \n        if self.epoch % self.N == 0:\n            name1 = 'monet_generator%02d.h5' % self.epoch\n            name2 = 'van_generator%02d.h5' % self.epoch\n            name3 = 'monet_discriminator%02d.h5' % self.epoch\n            name4 = 'van_discriminator%02d.h5' % self.epoch\n            monet_generator.save(name1)\n            van_generator.save(name2)\n            monet_discriminator.save(name3)\n            van_discriminator.save(name4)\n        self.epoch += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:20:58.625029Z","iopub.execute_input":"2022-03-12T05:20:58.625843Z","iopub.status.idle":"2022-03-12T05:20:58.638439Z","shell.execute_reply.started":"2022-03-12T05:20:58.625807Z","shell.execute_reply":"2022-03-12T05:20:58.637558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    lr_monet_gen = lambda: linear_schedule_with_warmup(tf.cast(monet_generator_optimizer.iterations, tf.float32))\n    lr_van_gen = lambda: linear_schedule_with_warmup(tf.cast(van_generator_optimizer.iterations, tf.float32))\n    \n    monet_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_monet_gen, beta_1=0.5)\n    van_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_van_gen, beta_1=0.5)\n\n    # Create discriminators\n    lr_monet_disc = lambda: linear_schedule_with_warmup(tf.cast(monet_discriminator_optimizer.iterations, tf.float32))\n    lr_van_disc = lambda: linear_schedule_with_warmup(tf.cast(van_discriminator_optimizer.iterations, tf.float32))\n    \n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_monet_disc, beta_1=0.5)\n    van_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_van_disc, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:01.128296Z","iopub.execute_input":"2022-03-12T05:21:01.128929Z","iopub.status.idle":"2022-03-12T05:21:01.137657Z","shell.execute_reply.started":"2022-03-12T05:21:01.128892Z","shell.execute_reply":"2022-03-12T05:21:01.137057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, van_generator, monet_discriminator, van_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = van_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = van_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:49.234199Z","iopub.execute_input":"2022-03-12T05:21:49.234515Z","iopub.status.idle":"2022-03-12T05:21:49.299511Z","shell.execute_reply.started":"2022-03-12T05:21:49.234475Z","shell.execute_reply":"2022-03-12T05:21:49.298328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    history = cycle_gan_model.fit(full_dataset,\n                        callbacks=[GANMonitor()],\n                        epochs=EPOCHS,\n                        steps_per_epoch=(max(n_monet_samples, n_van_samples)//BATCH_SIZE), \n                        verbose=1).history","metadata":{"execution":{"iopub.status.busy":"2022-03-12T05:21:50.746679Z","iopub.execute_input":"2022-03-12T05:21:50.746971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.keys()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T22:49:15.931071Z","iopub.execute_input":"2022-03-11T22:49:15.931369Z","iopub.status.idle":"2022-03-11T22:49:15.938738Z","shell.execute_reply.started":"2022-03-11T22:49:15.931337Z","shell.execute_reply":"2022-03-11T22:49:15.937857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Loss","metadata":{}},{"cell_type":"code","source":"plt.plot(np.append(np.mean(np.array(history['monet_gen_loss']),axis = (1,2,3)),np.mean(np.array(temphistoy['monet_gen_loss']),axis = (1,2,3))))\nplt.plot(np.append(np.mean(np.array(history['van_gen_loss']),axis = (1,2,3)),np.mean(np.array(temphistoy['van_gen_loss']),axis = (1,2,3))))\nplt.title('Generator Loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['Van Gogh', 'photo'], loc='upper left')\nplt.show()\n\n\nplt.plot(np.append(np.mean(np.array(history['monet_disc_loss']),axis = (1,2,3)),np.mean(np.array(temphistoy['monet_disc_loss']),axis = (1,2,3))))\nplt.plot(np.append(np.mean(np.array(history['van_disc_loss']),axis = (1,2,3)),np.mean(np.array(temphistoy['van_disc_loss']),axis = (1,2,3))))\nplt.title('Discriminator Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Van Gogh', 'photo'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:06:49.635898Z","iopub.execute_input":"2022-03-12T00:06:49.636234Z","iopub.status.idle":"2022-03-12T00:06:50.257267Z","shell.execute_reply.started":"2022-03-12T00:06:49.636204Z","shell.execute_reply":"2022-03-12T00:06:50.256372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the Training Result","metadata":{}},{"cell_type":"code","source":"def evaluate_cycle(ds, generator_a, generator_b, n_samples=1):\n    fig, axes = plt.subplots(n_samples, 3, figsize=(22, (n_samples*6)))\n    axes = axes.flatten()\n    \n    ds_iter = iter(ds)\n    for n_sample in range(n_samples):\n        idx = n_sample*3\n        example_sample = next(ds_iter)\n        generated_a_sample = generator_a.predict(example_sample)\n        generated_b_sample = generator_b.predict(generated_a_sample)\n        \n        axes[idx].set_title('Input image', fontsize=18)\n        axes[idx].imshow(example_sample[0] * 0.5 + 0.5)\n        axes[idx].axis('off')\n        \n        axes[idx+1].set_title('Generated image', fontsize=18)\n        axes[idx+1].imshow(generated_a_sample[0] * 0.5 + 0.5)\n        axes[idx+1].axis('off')\n        \n        axes[idx+2].set_title('Cycled image', fontsize=18)\n        axes[idx+2].imshow(generated_b_sample[0] * 0.5 + 0.5)\n        axes[idx+2].axis('off')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T22:50:38.936888Z","iopub.execute_input":"2022-03-11T22:50:38.937172Z","iopub.status.idle":"2022-03-11T22:50:38.947306Z","shell.execute_reply.started":"2022-03-11T22:50:38.937144Z","shell.execute_reply":"2022-03-11T22:50:38.946277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(van_ds.take(5), monet_generator, van_generator, n_samples=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:15:42.235063Z","iopub.execute_input":"2022-03-12T00:15:42.235383Z","iopub.status.idle":"2022-03-12T00:16:20.171835Z","shell.execute_reply.started":"2022-03-12T00:15:42.235343Z","shell.execute_reply":"2022-03-12T00:16:20.171093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(monet_ds.take(5), van_generator,monet_generator, n_samples=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:07:46.240514Z","iopub.execute_input":"2022-03-12T00:07:46.241043Z","iopub.status.idle":"2022-03-12T00:07:53.986651Z","shell.execute_reply.started":"2022-03-12T00:07:46.240965Z","shell.execute_reply":"2022-03-12T00:07:53.982119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"code","source":"monet_generator.save('vanphoto_generator.h5')\nvan_generator.save('photovan_generator.h5')\nmonet_discriminator.save('vanphoto_discriminator.h5')\nvan_discriminator.save('photovan_discriminator.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:08:45.187037Z","iopub.execute_input":"2022-03-12T00:08:45.187366Z","iopub.status.idle":"2022-03-12T00:08:46.567425Z","shell.execute_reply.started":"2022-03-12T00:08:45.18733Z","shell.execute_reply":"2022-03-12T00:08:46.56664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Trained Model for the Final Report","metadata":{}},{"cell_type":"markdown","source":"Here is our trained model, use it see the result","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = keras.models.load_model('/kaggle/input/cyclegan-model/monet_generator30.h5')\n    photo_monet_generator = keras.models.load_model('/kaggle/input/cyclegan-model/photo_generator30.h5')\n    monet_discriminator = keras.models.load_model('/kaggle/input/cyclegan-model/monet_discriminator30.h5')\n    photo_monet_discriminator = keras.models.load_model('/kaggle/input/cyclegan-model/photo_discriminator30.h5')    \n    van_generator = keras.models.load_model('/kaggle/input/cyclegan-model/vanphoto_generator1.h5')\n    photo_van_generator = keras.models.load_model('/kaggle/input/cyclegan-model/photovan_generator1.h5')\n    van_discriminator = keras.models.load_model('/kaggle/input/cyclegan-model/vanphoto_discriminator1.h5')\n    photo_van_monet_discriminator = keras.models.load_model('/kaggle/input/cyclegan-model/photovan_discriminator1.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(monet_ds.take(25), photo_monet_generator, photo_van_generator, n_samples=25)","metadata":{"execution":{"iopub.status.idle":"2022-03-12T00:46:00.602892Z","shell.execute_reply.started":"2022-03-12T00:45:23.221508Z","shell.execute_reply":"2022-03-12T00:46:00.599237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(van_ds.take(25), van_generator, monet_generator, n_samples=25)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:55:45.550407Z","iopub.execute_input":"2022-03-12T00:55:45.55075Z","iopub.status.idle":"2022-03-12T00:56:24.12341Z","shell.execute_reply.started":"2022-03-12T00:55:45.550699Z","shell.execute_reply":"2022-03-12T00:56:24.122393Z"},"trusted":true},"execution_count":null,"outputs":[]}]}