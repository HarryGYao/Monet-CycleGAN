{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction and Setup\n\nThis notebook utilizes a CycleGAN architecture to add Monet-style to photos. For this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU.\n\nFor more information, check out [TensorFlow](https://www.tensorflow.org/tutorials/generative/cyclegan) and [Keras](https://keras.io/examples/generative/cyclegan/) CycleGAN documentation pages.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, random, json, PIL, shutil, re, imageio, glob\nfrom tensorflow.keras.callbacks import Callback\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    \nprint(tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-17T17:36:25.737969Z","iopub.execute_input":"2021-07-17T17:36:25.73839Z","iopub.status.idle":"2021-07-17T17:36:35.609865Z","shell.execute_reply.started":"2021-07-17T17:36:25.738349Z","shell.execute_reply":"2021-07-17T17:36:35.60864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in the data\n\nWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords.","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:35.61162Z","iopub.execute_input":"2021-07-17T17:36:35.611956Z","iopub.status.idle":"2021-07-17T17:36:36.188014Z","shell.execute_reply.started":"2021-07-17T17:36:35.611922Z","shell.execute_reply":"2021-07-17T17:36:36.187085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.io.gfile.glob: Returns a list of files that match the given pattern(s).此处返回所有 .tfrec 格式的文件\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n\n# Obtain two lists of files that match the given patterns specified in str()\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n\nn_monet_samples = count_data_items(MONET_FILENAMES)\nn_photo_samples = count_data_items(PHOTO_FILENAMES)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.189664Z","iopub.execute_input":"2021-07-17T17:36:36.189986Z","iopub.status.idle":"2021-07-17T17:36:36.702289Z","shell.execute_reply.started":"2021-07-17T17:36:36.189956Z","shell.execute_reply":"2021-07-17T17:36:36.701319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\nBUFFER_SIZE = 1000\nBATCH_SIZE =  1\nEPOCHS_NUM = 30\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\n\ndef decode_image(image): \n    image = tf.image.decode_jpeg(image, channels=3)#Decode a JPEG-encoded image to a uint8 tensor. contents(此处image):A Tensor of type string,所以下面用的tf.string\n    image = (tf.cast(image, tf.float32) / 127.5) - 1 #unit8: Unsigned Integers of 8 bits. 可能后面必须是float32不然整数没法学习？\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])#\"*\"是去掉中括号的意思，*IMAGE_SIZE相当于就是 256，256； [*IMAGE_SIZE, 3]就是 [256，256, 3]；而[*IMAGE_SIZE, 3] 是[[256，256], 3]\n    return image\n\ndef read_tfrecord(example): #貌似因为我们用的 .tfrec文件，我们先decode成jpeg格式，再由decode_image转化成float32?\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string), #Configuration for parsing a fixed-length input feature. 这里是 string的list?\n        \"image\": tf.io.FixedLenFeature([], tf.string), #（划掉）不是很懂，这里的tf应该会被什么替代掉？ 就像self.io...一类的（划掉），看下面代码应该是强制要求的格式而已\n        \"target\": tf.io.FixedLenFeature([], tf.string)#{}是dictionary，这应该是储存图片信息\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)#（A scalar string Tensor, a single serialized Example，A dict mapping feature keys to FixedLenFeature or VarLenFeature values.）\n    #Return A dict mapping feature keys to Tensor and SparseTensor values. 所有这里返回了关于某张图的dict信息\n    image = decode_image(example['image'])#解码成float32格式,\n    return image#只返回图片，其他tag不要","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.704133Z","iopub.execute_input":"2021-07-17T17:36:36.704423Z","iopub.status.idle":"2021-07-17T17:36:36.71281Z","shell.execute_reply.started":"2021-07-17T17:36:36.704394Z","shell.execute_reply":"2021-07-17T17:36:36.711761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image):\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    # Apply jitter\n    if p_crop > .5:\n        image = tf.image.resize(image, [286, 286])\n        image = tf.image.random_crop(image, size=[256, 256, 3])\n        if p_crop > .9:\n            image = tf.image.resize(image, [300, 300])\n            image = tf.image.random_crop(image, size=[256, 256, 3])\n    \n    # Random rotation\n    if p_rotate > .9:\n        image = tf.image.rot90(image, k=3) # rotate 270º\n    elif p_rotate > .7:\n        image = tf.image.rot90(image, k=2) # rotate 180º\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=1) # rotate 90º\n    \n    # Random mirroring\n    if p_spatial > .6:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        if p_spatial > .9:\n            image = tf.image.transpose(image)\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.714547Z","iopub.execute_input":"2021-07-17T17:36:36.715021Z","iopub.status.idle":"2021-07-17T17:36:36.733051Z","shell.execute_reply.started":"2021-07-17T17:36:36.714975Z","shell.execute_reply":"2021-07-17T17:36:36.731737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the function to extract the image from the files.","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames) #A Dataset comprising records from one or more TFRecord files. dataset: tf.data.Dataset\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)#好家伙还是得看看TPU是个啥。 但先猜这行是用来TPU提速的。\n    #细说一下就是 map(map_func, num_parallel_calls=None, deterministic=None)， map_func就是上面那个cell的内容，这里就是把转格式读数据。然后\n    #(Optional.) A tf.int64 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. \n    #If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.（我们使用TPU加速）\n    #因为 map_func相当于是对当前对象的规则，所以在 read_tfrecord里面我们用的 tf.io.parse_single_example，对单一图片进行处理\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.734344Z","iopub.execute_input":"2021-07-17T17:36:36.734667Z","iopub.status.idle":"2021-07-17T17:36:36.750189Z","shell.execute_reply.started":"2021-07-17T17:36:36.734626Z","shell.execute_reply":"2021-07-17T17:36:36.748867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load in our datasets.","metadata":{}},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1) #batch(batch_size, ...) size = 1就是全放一起\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.751863Z","iopub.execute_input":"2021-07-17T17:36:36.752206Z","iopub.status.idle":"2021-07-17T17:36:36.873617Z","shell.execute_reply.started":"2021-07-17T17:36:36.752172Z","shell.execute_reply":"2021-07-17T17:36:36.872552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1):\n\n    monet_ds = load_dataset(monet_files)\n    photo_ds = load_dataset(photo_files)\n    \n    if augment:\n        monet_ds = monet_ds.map(augment, num_parallel_calls=AUTOTUNE)\n        photo_ds = photo_ds.map(augment, num_parallel_calls=AUTOTUNE)\n\n    if repeat:\n        monet_ds = monet_ds.repeat()\n        photo_ds = photo_ds.repeat()\n        \n    if shuffle:\n        monet_ds = monet_ds.shuffle(2048)\n        photo_ds = photo_ds.shuffle(2048)\n        \n    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)\n    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)\n    monet_ds = monet_ds.cache()\n    photo_ds = photo_ds.cache()\n    monet_ds = monet_ds.prefetch(AUTOTUNE)\n    photo_ds = photo_ds.prefetch(AUTOTUNE)\n    \n    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n    \n    return gan_ds","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.876083Z","iopub.execute_input":"2021-07-17T17:36:36.876384Z","iopub.status.idle":"2021-07-17T17:36:36.884626Z","shell.execute_reply.started":"2021-07-17T17:36:36.876355Z","shell.execute_reply":"2021-07-17T17:36:36.883595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_dataset = get_gan_dataset(MONET_FILENAMES, PHOTO_FILENAMES, augment=data_augment, repeat=True, shuffle=True, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:36.886922Z","iopub.execute_input":"2021-07-17T17:36:36.887531Z","iopub.status.idle":"2021-07-17T17:36:37.463356Z","shell.execute_reply.started":"2021-07-17T17:36:36.887482Z","shell.execute_reply":"2021-07-17T17:36:37.462335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet , example_photo = next(iter(full_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:37.465155Z","iopub.execute_input":"2021-07-17T17:36:37.46558Z","iopub.status.idle":"2021-07-17T17:36:43.269129Z","shell.execute_reply.started":"2021-07-17T17:36:37.465536Z","shell.execute_reply":"2021-07-17T17:36:43.268086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's  visualize a photo example and a Monet example.","metadata":{}},{"cell_type":"code","source":"def view_image(ds, nrows=1, ncols=5):\n    ds_iter = iter(ds)\n    # image = next(iter(ds)) # extract 1 from the dataset\n    # image = image.numpy()  # convert the image tensor to NumPy ndarrays.\n\n    fig = plt.figure(figsize=(25, nrows * 5.05 )) # figsize with Width, Height\n    \n    # loop thru all the images (number of rows * number of columns)\n    for i in range(ncols * nrows):\n        image = next(ds_iter)\n        image = image.numpy()\n        ax = fig.add_subplot(nrows, ncols, i+1, xticks=[], yticks=[])\n        ax.imshow(image[0] * 0.5 + .5) # rescale the data in [0, 1] for display","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:43.270561Z","iopub.execute_input":"2021-07-17T17:36:43.27099Z","iopub.status.idle":"2021-07-17T17:36:43.278605Z","shell.execute_reply.started":"2021-07-17T17:36:43.27095Z","shell.execute_reply":"2021-07-17T17:36:43.277315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_image(monet_ds,2, 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:43.280279Z","iopub.execute_input":"2021-07-17T17:36:43.280697Z","iopub.status.idle":"2021-07-17T17:36:45.108902Z","shell.execute_reply.started":"2021-07-17T17:36:43.280654Z","shell.execute_reply":"2021-07-17T17:36:45.107963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_image(photo_ds,2, 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:45.109968Z","iopub.execute_input":"2021-07-17T17:36:45.11046Z","iopub.status.idle":"2021-07-17T17:36:46.534029Z","shell.execute_reply.started":"2021-07-17T17:36:45.110429Z","shell.execute_reply":"2021-07-17T17:36:46.533164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the generator\n\nWe'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n\nThe `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n\nWe'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons.","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size,stride=2, apply_instancenorm=True):#filter = Integer, the dimensionality of the output space,size = kernel size普通卷积 大变小\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02) #Initializer that generates tensors with a normal distribution\n \n    result = keras.Sequential() #Sequential groups a linear stack of layers into a tf.keras.Model.\n    #from tensorflow.keras import layers\n    result.add(layers.Conv2D(filters, size, strides=stride, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)) #Instance normalization layer. Inherits From: GroupNormalization\n        #tfa.layers.GroupNormalization:Group normalization layer.\n        #gamma weight 搞不懂，应该是统计学上的东西吧，gamma distribution？\n\n    result.add(layers.LeakyReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:46.535178Z","iopub.execute_input":"2021-07-17T17:36:46.535592Z","iopub.status.idle":"2021-07-17T17:36:46.541976Z","shell.execute_reply.started":"2021-07-17T17:36:46.535562Z","shell.execute_reply":"2021-07-17T17:36:46.541243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Upsample` does the opposite of downsample and increases the dimensions of the of the image. `Conv2DTranspose` does basically the opposite of a `Conv2D` layer.","metadata":{}},{"cell_type":"code","source":"def upsample(filters, size,stride=2, apply_dropout=False):#小变大\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    #tf.keras.layers.Conv2DTranspose: Transposed convolution layer (sometimes called Deconvolution).\n    result.add(layers.Conv2DTranspose(filters, size, strides=stride, \n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:46.543491Z","iopub.execute_input":"2021-07-17T17:36:46.543959Z","iopub.status.idle":"2021-07-17T17:36:46.560178Z","shell.execute_reply.started":"2021-07-17T17:36:46.543928Z","shell.execute_reply":"2021-07-17T17:36:46.559222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build our generator!\n\nThe generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion.","metadata":{}},{"cell_type":"code","source":"def Generator():\n    inputs = layers.Input(shape=[256,256,3]) #图片格式 256*256*3\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 7, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(128, 4), # (bs, 32, 32, 128)\n        downsample(256, 4), # (bs, 16, 16, 256)\n        downsample(256, 4), # (bs, 8, 8, 256)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(1024, 4), # (bs, 1, 1, 512)\n    ]#叠了很多个conv2d layer\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 512+512(skip connection))\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(256, 4, apply_dropout=True), # (bs, 8, 8, 512)\n        upsample(256, 4), # (bs, 16, 16, 512)\n        upsample(128, 4), # (bs, 32, 32, 256)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]#叠了很多个Tansposeconv2d layer\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 7,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)#还原\n\n    x = inputs\n\n    # Downsampling through the model\n    skip_connection = 0\n    skips = []\n    for down in down_stack: #down 是一个实例layer，只用传图(x)进去了\n        x = down(x) #往下卷！\n        skips.append(x)\n    skips = reversed(skips[:-1])#为啥要reverse。。。因为skip connections中，同层的down和up相互对应，上面俩是反着的。。。\n    \n    # Upsampling and establishing the skip connections\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)#往上还原！\n        if skip_connection == 0:\n            x = layers.Concatenate()([x, skip])#只拼接downsampling前的\n            skip_connection = 1\n        else:\n            skip_connection = 0\n\n    x = last(x)#生成预测，和input一层，所以单独放在外面，上面的都是卷积层。\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:46.561462Z","iopub.execute_input":"2021-07-17T17:36:46.561937Z","iopub.status.idle":"2021-07-17T17:36:46.57483Z","shell.execute_reply.started":"2021-07-17T17:36:46.561903Z","shell.execute_reply":"2021-07-17T17:36:46.573797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the discriminator\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4,2, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4,2)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4,2)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) #不懂在干什么。。。。只知道在加了些0，也许是为了凑34？\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n    #是否需要DENSE layer,研究下\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:46.57606Z","iopub.execute_input":"2021-07-17T17:36:46.576578Z","iopub.status.idle":"2021-07-17T17:36:46.594886Z","shell.execute_reply.started":"2021-07-17T17:36:46.576534Z","shell.execute_reply":"2021-07-17T17:36:46.593641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): #和try catch 一个意思的样子, strategy应该TPU的运行环境\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:46.596197Z","iopub.execute_input":"2021-07-17T17:36:46.596653Z","iopub.status.idle":"2021-07-17T17:36:55.872632Z","shell.execute_reply.started":"2021-07-17T17:36:46.596615Z","shell.execute_reply":"2021-07-17T17:36:55.871666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our generators are not trained yet, the generated Monet-esque photo does not show what is expected at this point.","metadata":{}},{"cell_type":"code","source":"to_monet = monet_generator(example_photo)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Monet-esque Photo\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:55.874146Z","iopub.execute_input":"2021-07-17T17:36:55.874565Z","iopub.status.idle":"2021-07-17T17:36:56.458378Z","shell.execute_reply.started":"2021-07-17T17:36:55.874522Z","shell.execute_reply":"2021-07-17T17:36:56.457271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the CycleGAN model\n\nWe will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n\nThe losses are defined in the next section.","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__() #super(CycleGan, self) 和 super（）应该是一样的，“In Python 3, the super(Square, self) call is equivalent to the parameterless super() call.”\n        self.m_gen = monet_generator#只是说 super(CycleGan, self)，CycleGan可以换成其他，相当于可以调用其他类的 super()\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile( #Configures the model for training.\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape: #Record operations for automatic differentiation. persistent=True 是让微分留在内存，不然会被garbage collected.\n            #Boolean controlling whether a persistent gradient tape is created. False by default, \n            #which means at most one call can be made to the gradient() method on this object.\n            \n            # photo to monet back to photo  \n            fake_monet = self.m_gen(real_photo, training=True)#真照片 ->假莫奈画\n            cycled_photo = self.p_gen(fake_monet, training=True)#假莫奈画->还原照片\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)#真莫奈画 ->假照片\n            cycled_monet = self.m_gen(fake_photo, training=True)#假照片 ->还原莫奈画\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)#真莫奈画 ->假莫奈画\n            same_photo = self.p_gen(real_photo, training=True)#真照片->假照片\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)#判断真莫奈画 \n            disc_real_photo = self.p_disc(real_photo, training=True)#判断真照片\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)#判断假莫奈画 \n            disc_fake_photo = self.p_disc(fake_photo, training=True)#判断假照片\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.45985Z","iopub.execute_input":"2021-07-17T17:36:56.460154Z","iopub.status.idle":"2021-07-17T17:36:56.4794Z","shell.execute_reply.started":"2021-07-17T17:36:56.460125Z","shell.execute_reply":"2021-07-17T17:36:56.478141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define loss functions\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        #真的都是1,所以和 tf.ones_like 对比\n        #real_loss = tf.square(tf.ones_like(real) - real)\n        \n        #假的都是0,所以和 tf.zeros_like 对比     \n\n        #generated_loss = tf.square(generated)\n\n        #total_disc_loss = real_loss + generated_loss\n        \n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n\n        return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.480956Z","iopub.execute_input":"2021-07-17T17:36:56.48127Z","iopub.status.idle":"2021-07-17T17:36:56.496298Z","shell.execute_reply.started":"2021-07-17T17:36:56.481242Z","shell.execute_reply":"2021-07-17T17:36:56.495434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n        #我们希望discriminator都给1，所以和1's比\n       # return tf.square(tf.ones_like(generated) - generated)\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.497636Z","iopub.execute_input":"2021-07-17T17:36:56.497962Z","iopub.status.idle":"2021-07-17T17:36:56.519963Z","shell.execute_reply.started":"2021-07-17T17:36:56.497932Z","shell.execute_reply":"2021-07-17T17:36:56.518689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        #loss1 = tf.reduce_mean(tf.square(real_image - cycled_image))\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.523893Z","iopub.execute_input":"2021-07-17T17:36:56.524276Z","iopub.status.idle":"2021-07-17T17:36:56.532875Z","shell.execute_reply.started":"2021-07-17T17:36:56.524237Z","shell.execute_reply":"2021-07-17T17:36:56.531642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        #loss = tf.reduce_mean(tf.square(real_image - same_image))\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.534438Z","iopub.execute_input":"2021-07-17T17:36:56.534897Z","iopub.status.idle":"2021-07-17T17:36:56.54549Z","shell.execute_reply.started":"2021-07-17T17:36:56.534862Z","shell.execute_reply":"2021-07-17T17:36:56.544131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the CycleGAN\n\nLet's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model.","metadata":{}},{"cell_type":"code","source":"# Callbacks\nclass GANMonitor(Callback):\n    \"\"\"A callback to generate and save images after each epoch\"\"\"\n\n    def __init__(self, num_img=1, monet_path='monet', photo_path='photo'):\n        self.num_img = num_img\n        self.monet_path = monet_path\n        self.photo_path = photo_path\n        # Create directories to save the generate images\n        if not os.path.exists(self.monet_path):\n            os.makedirs(self.monet_path)\n        if not os.path.exists(self.photo_path):\n            os.makedirs(self.photo_path)\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Monet generated images\n        for i, img in enumerate(photo_ds.take(self.num_img)):\n            prediction = monet_generator(img, training=False)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            prediction = PIL.Image.fromarray(prediction)\n            prediction.save(f'{self.monet_path}/generated_{i}_{epoch+1}.png')\n            \n        # Photo generated images\n        for i, img in enumerate(monet_ds.take(self.num_img)):\n            prediction = photo_generator(img, training=False)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            prediction = PIL.Image.fromarray(prediction)\n            prediction.save(f'{self.photo_path}/generated_{i}_{epoch+1}.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.546946Z","iopub.execute_input":"2021-07-17T17:36:56.547314Z","iopub.status.idle":"2021-07-17T17:36:56.559207Z","shell.execute_reply.started":"2021-07-17T17:36:56.547223Z","shell.execute_reply":"2021-07-17T17:36:56.558157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.56059Z","iopub.execute_input":"2021-07-17T17:36:56.561165Z","iopub.status.idle":"2021-07-17T17:36:56.57787Z","shell.execute_reply.started":"2021-07-17T17:36:56.561125Z","shell.execute_reply":"2021-07-17T17:36:56.576749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:36:56.579225Z","iopub.execute_input":"2021-07-17T17:36:56.579834Z","iopub.status.idle":"2021-07-17T17:36:56.652419Z","shell.execute_reply.started":"2021-07-17T17:36:56.579778Z","shell.execute_reply":"2021-07-17T17:36:56.650858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cycle_gan_model.fit(full_dataset,\n                    callbacks=[GANMonitor()],\n                    epochs=60,\n                    steps_per_epoch=(max(n_monet_samples, n_photo_samples)//4), \n                    verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:37:32.913373Z","iopub.execute_input":"2021-07-17T16:37:32.913792Z","iopub.status.idle":"2021-07-17T16:37:33.594417Z","shell.execute_reply.started":"2021-07-17T16:37:32.913759Z","shell.execute_reply":"2021-07-17T16:37:33.591995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize our Monet-esque photos","metadata":{}},{"cell_type":"code","source":"def evaluate_cycle(ds, generator_a, generator_b, n_samples=1):\n    fig, axes = plt.subplots(n_samples, 3, figsize=(22, (n_samples*6)))\n    axes = axes.flatten()\n    \n    ds_iter = iter(ds)\n    for n_sample in range(n_samples):\n        idx = n_sample*3\n        example_sample = next(ds_iter)\n        generated_a_sample = generator_a.predict(example_sample)\n        generated_b_sample = generator_b.predict(generated_a_sample)\n        \n        axes[idx].set_title('Input image', fontsize=18)\n        axes[idx].imshow(example_sample[0] * 0.5 + 0.5)\n        axes[idx].axis('off')\n        \n        axes[idx+1].set_title('Generated image', fontsize=18)\n        axes[idx+1].imshow(generated_a_sample[0] * 0.5 + 0.5)\n        axes[idx+1].axis('off')\n        \n        axes[idx+2].set_title('Cycled image', fontsize=18)\n        axes[idx+2].imshow(generated_b_sample[0] * 0.5 + 0.5)\n        axes[idx+2].axis('off')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:49:55.33512Z","iopub.execute_input":"2021-07-17T17:49:55.335485Z","iopub.status.idle":"2021-07-17T17:49:55.345581Z","shell.execute_reply.started":"2021-07-17T17:49:55.335456Z","shell.execute_reply":"2021-07-17T17:49:55.34434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(photo_ds.take(15), monet_generator, photo_generator, n_samples=15)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:49:57.407652Z","iopub.execute_input":"2021-07-17T17:49:57.408068Z","iopub.status.idle":"2021-07-17T17:50:36.121296Z","shell.execute_reply.started":"2021-07-17T17:49:57.408031Z","shell.execute_reply":"2021-07-17T17:50:36.120341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_cycle(monet_ds.take(15), photo_generator,monet_generator, n_samples=15)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:50:36.122487Z","iopub.execute_input":"2021-07-17T17:50:36.122904Z","iopub.status.idle":"2021-07-17T17:50:56.985818Z","shell.execute_reply.started":"2021-07-17T17:50:36.122866Z","shell.execute_reply":"2021-07-17T17:50:56.983831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"code","source":"monet_generator.save('monet_generator.h5')\nphoto_generator.save('photo_generator.h5')\nmonet_discriminator.save('monet_discriminator.h5')\nphoto_discriminator.save('photo_discriminator.h5')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:47:34.648551Z","iopub.status.idle":"2021-07-15T12:47:34.64922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Monet generation GIF","metadata":{}},{"cell_type":"code","source":"def create_gif(images_path, gif_path):\n    images = []\n    filenames = glob.glob(images_path)\n    filenames.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n    for epoch, filename in enumerate(filenames):\n        img = PIL.ImageDraw.Image.open(filename)\n        ImageDraw.Draw(img).text((0, 0),  # Coordinates\n                                 f'Epoch {epoch+1}')\n        images.append(img)\n    imageio.mimsave(gif_path, images, fps=2) # Save gif","metadata":{"execution":{"iopub.status.busy":"2021-07-15T13:59:28.715703Z","iopub.execute_input":"2021-07-15T13:59:28.716222Z","iopub.status.idle":"2021-07-15T13:59:28.725891Z","shell.execute_reply.started":"2021-07-15T13:59:28.716167Z","shell.execute_reply":"2021-07-15T13:59:28.724763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageDraw\ncreate_gif('/kaggle/working/photo/*.png', 'photo.gif') ","metadata":{"execution":{"iopub.status.busy":"2021-07-15T14:00:50.623987Z","iopub.execute_input":"2021-07-15T14:00:50.624466Z","iopub.status.idle":"2021-07-15T14:00:51.280203Z","shell.execute_reply.started":"2021-07-15T14:00:50.624428Z","shell.execute_reply":"2021-07-15T14:00:51.279194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_gif('/kaggle/working/monet/*.png', 'monet.gif')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T13:59:29.50822Z","iopub.execute_input":"2021-07-15T13:59:29.508563Z","iopub.status.idle":"2021-07-15T13:59:30.025772Z","shell.execute_reply.started":"2021-07-15T13:59:29.508529Z","shell.execute_reply":"2021-07-15T13:59:30.024659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"def predict_and_save(input_ds, generator_model, output_path):\n    i = 1\n    for img in input_ds:\n        prediction = generator_model(img, training=False)[0].numpy() # make predition\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   # re-scale\n        im = PIL.Image.fromarray(prediction)\n        im.save(f'{output_path}{str(i)}.jpg')\n        i += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-15T11:15:32.81451Z","iopub.execute_input":"2021-07-15T11:15:32.815184Z","iopub.status.idle":"2021-07-15T11:15:32.820648Z","shell.execute_reply.started":"2021-07-15T11:15:32.81515Z","shell.execute_reply":"2021-07-15T11:15:32.819969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nos.makedirs('../images/') # Create folder to save generated images\npredict_and_save(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, '../images/')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T11:15:35.678771Z","iopub.execute_input":"2021-07-15T11:15:35.67921Z","iopub.status.idle":"2021-07-15T11:39:31.504628Z","shell.execute_reply.started":"2021-07-15T11:15:35.679176Z","shell.execute_reply":"2021-07-15T11:39:31.503576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive('/kaggle/working/images/', 'zip', '../images')\n\nprint(f\"Number of generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T11:39:31.506268Z","iopub.execute_input":"2021-07-15T11:39:31.506527Z","iopub.status.idle":"2021-07-15T11:39:35.506945Z","shell.execute_reply.started":"2021-07-15T11:39:31.506503Z","shell.execute_reply":"2021-07-15T11:39:35.505984Z"},"trusted":true},"execution_count":null,"outputs":[]}]}